service 是对一组pod的抽象，他会根据访问策略来访问这组pod。
kubernetes在创建服务时会为服务分配一个虚拟的ip地址，客户端通过访问这个虚拟ip地址来访问服务，而服务则负责将请求转发到后端的pod上。

在kubernetes集群的每个node上都会运行一个kube-proxy服务进程，这个进程可以看作service的透明代理兼负载均衡器，其核心功能是将到某个service的请求转发到后端的多个pod实例上。对每一个tcp类型的kubernetes service，kube-proxy都会在本地node上建立一个socketserver来负责接收请求，然后均匀发送到后端某个pod的端口上，这个过程默认采用round robin负载均衡算法。另外kubernetes也提供通过修改service的service.spec.sessionaffinity参数的值来实现回话保持特性的定向转发，如果设置的值为“clientIP”,则将来自同一个clientIP的请求都转发到同一个后端pod上。


service的cluster IP与nodeport等概念是kube-proxy服务通过iptables的nat转换实现的，kube-proxy在运行过程中动态创建于service相关的iptables规则，这些实现了cluster ip 及nodeport的请求流量重定向到kube-proxy进程上对应服务的代理端口的功能。由于iptables机制针对的是本地的kube-proxy端口，所以每个node上都要运行kube-proxy组件，这样一来，在kubernetes集群内部，我们可以在任意node上发起对service的访问请求。

访问service的请求，不论是用cluster ip+targetport的方式，还是用节点机ip+nodeport的方式，都被节点机的iptables规则重定向到kube-proxy监听service服务代理的端口。kube-proxy接收到service的访问请求后，会如何选择后端node呢

首先，目前kube-proxy的负载均衡器只支持round robin算法。round robin算法在按照成员列表逐个选取成员，如果一轮循环完，便从头开始下一轮，如此循环往复。kube-proxy的负载均衡器在round robin算法的基础上还支持session保持。如果service在定义中指定了session保持，则kube-proxy接收请求时会从本地内存中查找是否存在来自改请求ip的affinitystate对象，如果存在该对象，且session没有超时，则kube-proxy将请求转向该affinitystate所指向的后端pod。如果本地存在没有来自该请求ip的affinitystate对象，则按照round robin算法为该请求挑选一个endpoint，并创建一个affinitystate对象，记录请求ip和指向的endpoint。后面的请求就会“黏连”到这个创建好的affinitystate对象上，，这就实现了客户端ip会话保持的功能。



kube-proxy通过查询和监听API server中service与endpoint的变化，为每个service都建立了一个服务代理对象，并自动同步。服务代理对象是kube-proxy程序内部的一种数据结构，它包括一个用于监听此服务请求的socketservic，socketserver的端口是随机选择的一个本地空闲端口。此外，kube-proxy内部也创建了一个负载均衡器---loadbalancer, loadbalancer上保存了service到对应的后端endpoint列表的动态转发路由表，而具体的路由选择则取决于round robin 负载均衡算法及service的session会话保持（sessionaffinity）这两个特性

针对发送变化的service列表，kube-proxy会逐个处理
1.如果该service没有设置集群ip，则不做处理，否则，获取该service的所有端口定义列表
2.逐个读取服务端口定义列表中的端口信息，根据端口名称、service名称和namespace判断本地是否已经存在对应的服务代理对象，如果不存在则新建；如果存在并且service端口呗修改过，则先删除iptables中和该service端口相关的规则，关闭服务代理对象，然后走新建流程，即为该service端口分配服务代理对象并为该service创建相关的iptables规则。
3.更新负载均衡器组件中对应service的转发地址列表，对于新建的service，确定转发时的会话保持策略
4.对于已经删除的service则进行清理


kube-proxy在启动时和监听到service或endpoint的变化后，会在本机iptables的nat表中添加4条规则链。

1.kube-portals-container: 从容器中通过service cluster ip 和端口号访问service的请求。
2.kube-portals-host 从主机中通过service cluster ip 和端口号访问service的请求
3.kube-nodeport-container 从容器中通过service cluster ip 和nodeport 端口号访问service请求
4.kube-nodeport-host： 从主机中通过service的nodeport的端口号访问service的请求

此外，kube-proxy在iptables中为每个service创建由clusterip+service 端口到kube-proxy所在主机ip+service 代理服务所监听的端口转发规则。转发规则的包匹配规则部分，如下所示：




